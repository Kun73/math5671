{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import collections\n",
    "import os\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk1.8.0_231\"\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\\\Users\\\\yizho\\\\Downloads\\\\spark-2.4.4-bin-hadoop2.7\\\\spark-2.4.4-bin-hadoop2.7\"\n",
    "os.environ['HADOOP_HOME'] = \"C:\\\\winutils\"\n",
    "conf = SparkConf().setAppName(\"MLlib\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+---------+------+-------------------+-------------------+\n",
      "|payment_id|customer_id|staff_id|rental_id|amount|       payment_date|        last_update|\n",
      "+----------+-----------+--------+---------+------+-------------------+-------------------+\n",
      "|         1|          1|       1|       76|  2.99|2005-05-25 11:30:37|2006-02-15 22:12:30|\n",
      "|         2|          1|       1|      573|  0.99|2005-05-28 10:35:23|2006-02-15 22:12:30|\n",
      "|         3|          1|       1|     1185|  5.99|2005-06-15 00:54:12|2006-02-15 22:12:30|\n",
      "+----------+-----------+--------+---------+------+-------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
      "|rental_id|        rental_date|inventory_id|customer_id|        return_date|staff_id|        last_update|\n",
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
      "|        1|2005-05-24 22:53:30|         367|        130|2005-05-26 22:04:30|       1|2006-02-15 21:30:53|\n",
      "|        2|2005-05-24 22:54:33|        1525|        459|2005-05-28 19:40:33|       1|2006-02-15 21:30:53|\n",
      "|        3|2005-05-24 23:03:39|        1711|        408|2005-06-01 22:12:39|       1|2006-02-15 21:30:53|\n",
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+----------+---------+----------+-------+--------------------+--------+------+--------+--------------------+--------------+\n",
      "|staff_id|first_name|last_name|address_id|picture|               email|store_id|active|username|            password|   last_update|\n",
      "+--------+----------+---------+----------+-------+--------------------+--------+------+--------+--------------------+--------------+\n",
      "|       1|      Mike|  Hillyer|         3|   NULL|Mike.Hillyer@saki...|       1|     1|    Mike|8cb2237d0679ca88d...|2/15/2006 3:57|\n",
      "|       2|       Jon| Stephens|         4|   NULL|Jon.Stephens@saki...|       2|     1|     Jon|                NULL|2/15/2006 3:57|\n",
      "+--------+----------+---------+----------+-------+--------------------+--------+------+--------+--------------------+--------------+\n",
      "\n",
      "+------------+-------+--------+-------------------+\n",
      "|inventory_id|film_id|store_id|        last_update|\n",
      "+------------+-------+--------+-------------------+\n",
      "|           1|      1|       1|2006-02-15 05:09:17|\n",
      "|           2|      1|       1|2006-02-15 05:09:17|\n",
      "|           3|      1|       1|2006-02-15 05:09:17|\n",
      "+------------+-------+--------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+----------------+----------+-------------------+\n",
      "|store_id|manager_staff_id|address_id|        last_update|\n",
      "+--------+----------------+----------+-------------------+\n",
      "|       1|               1|         1|2006-02-15 04:57:12|\n",
      "|       2|               2|         2|2006-02-15 04:57:12|\n",
      "+--------+----------------+----------+-------------------+\n",
      "\n",
      "+----------+------------------+--------+--------+-------+-----------+-----------+-------------------+\n",
      "|address_id|           address|address2|district|city_id|postal_code|      phone|        last_update|\n",
      "+----------+------------------+--------+--------+-------+-----------+-----------+-------------------+\n",
      "|         1| 47 MySakila Drive|    NULL| Alberta|    300|       null|       null|2014-09-25 22:30:27|\n",
      "|         2|28 MySQL Boulevard|    NULL|     QLD|    576|       null|       null|2014-09-25 22:30:09|\n",
      "|         3| 23 Workhaven Lane|    NULL| Alberta|    300|       null|14033335568|2014-09-25 22:30:27|\n",
      "+----------+------------------+--------+--------+-------+-----------+-----------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------+--------+----------+---------+--------------------+----------+------+-------------------+-------------------+\n",
      "|customer_id|store_id|first_name|last_name|               email|address_id|active|        create_date|        last_update|\n",
      "+-----------+--------+----------+---------+--------------------+----------+------+-------------------+-------------------+\n",
      "|          1|       1|      MARY|    SMITH|MARY.SMITH@sakila...|         5|     1|2006-02-14 22:04:36|2006-02-15 04:57:20|\n",
      "|          2|       1|  PATRICIA|  JOHNSON|PATRICIA.JOHNSON@...|         6|     1|2006-02-14 22:04:36|2006-02-15 04:57:20|\n",
      "|          3|       1|     LINDA| WILLIAMS|LINDA.WILLIAMS@sa...|         7|     1|2006-02-14 22:04:36|2006-02-15 04:57:20|\n",
      "+-----------+--------+----------+---------+--------------------+----------+------+-------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+----------------+--------------------+------------+-----------+--------------------+---------------+-----------+------+----------------+------+--------------------+-------------------+\n",
      "|film_id|           title|         description|release_year|language_id|original_language_id|rental_duration|rental_rate|length|replacement_cost|rating|    special_features|        last_update|\n",
      "+-------+----------------+--------------------+------------+-----------+--------------------+---------------+-----------+------+----------------+------+--------------------+-------------------+\n",
      "|      1|ACADEMY DINOSAUR|A Epic Drama of a...|        2006|          1|                NULL|              6|       0.99|    86|           20.99|    PG|Deleted Scenes,Be...|2006-02-15 05:03:42|\n",
      "|      2|  ACE GOLDFINGER|A Astounding Epis...|        2006|          1|                NULL|              3|       4.99|    48|           12.99|     G|Trailers,Deleted ...|2006-02-15 05:03:42|\n",
      "|      3|ADAPTATION HOLES|A Astounding Refl...|        2006|          1|                NULL|              7|       2.99|    50|           18.99| NC-17|Trailers,Deleted ...|2006-02-15 05:03:42|\n",
      "+-------+----------------+--------------------+------------+-----------+--------------------+---------------+-----------+------+----------------+------+--------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Homework solution start here follow by question order\n"
     ]
    }
   ],
   "source": [
    "payment_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r'C:\\Users\\yizho\\PycharmProjects\\HW9\\data\\payment.csv')\n",
    "payment_df = payment_df.alias(\"payment_df\")\n",
    "payment_df.show(3)\n",
    "rental_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r'C:\\Users\\yizho\\PycharmProjects\\HW9\\data\\rental.csv')\n",
    "rental_df = rental_df.alias('rental_df')\n",
    "rental_df.show(3)\n",
    "staff_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r'C:\\Users\\yizho\\PycharmProjects\\HW9\\data\\staff.csv')\n",
    "staff_df = staff_df.alias('staff_df')\n",
    "staff_df.show(3)\n",
    "inventory_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r'C:\\Users\\yizho\\PycharmProjects\\HW9\\data\\inventory.csv')\n",
    "inventory_df = inventory_df.alias('inventory_df')\n",
    "inventory_df.show(3)\n",
    "store_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r'C:\\Users\\yizho\\PycharmProjects\\HW9\\data\\store.csv')\n",
    "store_df = store_df.alias('store_df')\n",
    "store_df.show(3)\n",
    "address_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r'C:\\Users\\yizho\\PycharmProjects\\HW9\\data\\address.csv')\n",
    "address_df = address_df.alias('address_df')\n",
    "address_df.show(3)\n",
    "customer_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r'C:\\Users\\yizho\\PycharmProjects\\HW9\\data\\customer.csv')\n",
    "customer_df = customer_df.alias('customer_df')\n",
    "customer_df.show(3)\n",
    "film_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r'C:\\Users\\yizho\\PycharmProjects\\HW9\\data\\film.csv')\n",
    "film_df = film_df.alias('film_df')\n",
    "film_df.show(3)\n",
    "print(\"Homework solution start here follow by question order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|customer_id|      total_amount|\n",
      "+-----------+------------------+\n",
      "|        526| 221.5500000000001|\n",
      "|        148| 216.5400000000001|\n",
      "|        144|195.58000000000007|\n",
      "+-----------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payment_df.withColumn(\"amount\", payment_df[\"amount\"].cast(\"double\"))\n",
    "import pyspark.sql.functions as func\n",
    "#payment_df.groupby('customer_id').agg(func.sum(payment_df.amount).alias('total_amount')).orderBy('total_amount',ascending = False).show(3)\n",
    "payment_df.createOrReplaceTempView(\"payment_df\")\n",
    "spark.sql(\"select customer_id, sum(amount) as total_amount from payment_df group by customer_id order by total_amount DESC\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
      "|rental_id|        rental_date|inventory_id|customer_id|        return_date|staff_id|        last_update|\n",
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
      "|       32|2005-05-25 04:06:21|        3832|        230|2005-05-25 23:55:21|       1|2006-02-15 21:30:53|\n",
      "|       21|2005-05-25 01:59:46|         146|        388|2005-05-26 01:01:46|       2|2006-02-15 21:30:53|\n",
      "|       14|2005-05-25 00:31:15|        2701|        446|2005-05-26 02:56:15|       1|2006-02-15 21:30:53|\n",
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rental_df.createOrReplaceTempView(\"rental_df\")\n",
    "spark.sql(\"select * from rental_df where year(rental_date) = 2005 order by return_date\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|staff_id|count|\n",
      "+--------+-----+\n",
      "|       1| 8040|\n",
      "|       2| 8004|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select staff_id, count(rental_id) as count from rental_df group by staff_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+\n",
      "|first_name|last_name|count|\n",
      "+----------+---------+-----+\n",
      "|      Mike|  Hillyer| 8040|\n",
      "|       Jon| Stephens| 8004|\n",
      "+----------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staff_df.createOrReplaceTempView(\"staff_df\")\n",
    "spark.sql(\"select first(first_name) as first_name, first(last_name) as last_name, count(rental_id) as count from staff_df inner join rental_df on rental_df.staff_id = staff_df.staff_id group by staff_df.staff_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------+---------------+--------------------+----------+---------+----------+---------+\n",
      "|        rental_date|        return_date|           address|          title|         description|first_name|last_name|first_name|last_name|\n",
      "+-------------------+-------------------+------------------+---------------+--------------------+----------+---------+----------+---------+\n",
      "|2005-05-24 22:53:30|2005-05-26 22:04:30| 47 MySakila Drive|BLANKET BEVERLY|A Emotional Docum...| CHARLOTTE|   HUNTER|      Mike|  Hillyer|\n",
      "|2005-05-24 22:54:33|2005-05-28 19:40:33|28 MySQL Boulevard|   FREAKY POCUS|A Fast-Paced Docu...|     TOMMY|  COLLAZO|      Mike|  Hillyer|\n",
      "|2005-05-24 23:03:39|2005-06-01 22:12:39|28 MySQL Boulevard|  GRADUATE LORD|A Lacklusture Epi...|    MANUEL|  MURRELL|      Mike|  Hillyer|\n",
      "+-------------------+-------------------+------------------+---------------+--------------------+----------+---------+----------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inventory_df.createOrReplaceTempView(\"inventory_df\")\n",
    "store_df.createOrReplaceTempView(\"store_df\")\n",
    "address_df.createOrReplaceTempView(\"address_df\")\n",
    "customer_df.createOrReplaceTempView(\"customer_df\")\n",
    "film_df.createOrReplaceTempView(\"film_df\")\n",
    "spark.sql(\"\"\"select r.rental_date, r.return_date, a.address, f.title, f.description, c.first_name, c.last_name, s.first_name, s.last_name\n",
    "from rental_df as r\n",
    "join inventory_df as i\n",
    "on r.inventory_id = i.inventory_id\n",
    "join store_df as st\n",
    "on i.store_id = st.store_id\n",
    "join address_df as a\n",
    "on st.address_id = a.address_id\n",
    "join customer_df as c\n",
    "on r.customer_id = c.customer_id\n",
    "join staff_df as s\n",
    "on s.store_id = c.store_id\n",
    "join film_df as f\n",
    "on i.film_id = f.film_id\n",
    "order by r.rental_date\n",
    " \"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|customer_id|     total_amount|\n",
      "+-----------+-----------------+\n",
      "|        526|221.5500000000001|\n",
      "|        148|216.5400000000001|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select customer_id, total_amount from\n",
    "(select customer_id, sum(amount) as total_amount from payment_df where year(payment_date) =2005 group by customer_id)\n",
    "where total_amount > 200\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+\n",
      "|first_name|last_name|     total_amount|\n",
      "+----------+---------+-----------------+\n",
      "|      KARL|     SEAL|221.5500000000001|\n",
      "|   ELEANOR|     HUNT|216.5400000000001|\n",
      "+----------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.createOrReplaceTempView(\"customer_df\")\n",
    "spark.sql(\"\"\"select * from\n",
    "(select first(customer_df.first_name) as first_name, first(customer_df.last_name) as last_name, sum(amount) as total_amount from payment_df\n",
    "inner join customer_df\n",
    "on customer_df.customer_id = payment_df.customer_id where year(payment_date) =2005 group by payment_df.customer_id)\n",
    "where total_amount > 200 \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+--------+\n",
      "|customer_id_in_2005_not_in_2006|rentyear|\n",
      "+-------------------------------+--------+\n",
      "|                            413|    2005|\n",
      "|                            377|    2005|\n",
      "|                            406|    2005|\n",
      "|                            148|    2005|\n",
      "|                            463|    2005|\n",
      "|                             26|    2005|\n",
      "|                            358|    2005|\n",
      "|                             62|    2005|\n",
      "|                            285|    2005|\n",
      "|                            392|    2005|\n",
      "+-------------------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------------------------+\n",
      "|customer_id_in_2005_not_in_2006|\n",
      "+-------------------------------+\n",
      "|                            441|\n",
      "+-------------------------------+\n",
      "\n",
      "+-------------------------------+--------+\n",
      "|customer_id_in_2006_not_in_2005|rentyear|\n",
      "+-------------------------------+--------+\n",
      "+-------------------------------+--------+\n",
      "\n",
      "+-------------------------------+\n",
      "|customer_id_in_2006_not_in_2005|\n",
      "+-------------------------------+\n",
      "|                              0|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c2005 = spark.sql(\"select customer_id, year(rental_date) as rentyear from rental_df where year(rental_date) = 2005\")\n",
    "c2006 = spark.sql(\"select customer_id, year(rental_date) as rentyear from rental_df where year(rental_date) = 2006\")\n",
    "c2005.createOrReplaceTempView(\"c2005\")\n",
    "c2006.createOrReplaceTempView(\"c2006\")\n",
    "spark.sql(\"\"\"select distinct(c2005.customer_id) as customer_id_in_2005_not_in_2006, c2005.rentyear from c2005\n",
    "where c2005.customer_id not in (\n",
    "    select c2006.customer_id from c2006\n",
    ") \"\"\").show(10)\n",
    "spark.sql(\"\"\"select count(distinct(c2005.customer_id)) as customer_id_in_2005_not_in_2006 from c2005\n",
    "where c2005.customer_id not in (\n",
    "    select c2006.customer_id from c2006\n",
    ") \"\"\").show()\n",
    "spark.sql(\"\"\"select distinct(c2006.customer_id) as customer_id_in_2006_not_in_2005, c2006.rentyear from c2006\n",
    "where c2006.customer_id not in (\n",
    "    select c2005.customer_id from c2005\n",
    ") \"\"\").show(10)\n",
    "spark.sql(\"\"\"select count(c2006.customer_id) as customer_id_in_2006_not_in_2005 from c2006\n",
    "where c2006.customer_id not in (\n",
    "    select c2005.customer_id from c2005\n",
    ") \"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
